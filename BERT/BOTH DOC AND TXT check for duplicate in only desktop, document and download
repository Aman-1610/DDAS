#pip install python-docx
#
#
#

from transformers import AutoTokenizer, AutoModel
import os
import torch
from sklearn.metrics.pairwise import cosine_similarity
from docx import Document  # Import for reading .docx files

# Step 1: Load BERT Model and Tokenizer
model_name = "sentence-transformers/all-MiniLM-L6-v2"  # Optimized for semantic similarity
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)


def get_embedding(text):
    """Generate a sentence embedding for the input text using BERT."""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Use mean pooling to create a single vector
    embedding = outputs.last_hidden_state.mean(dim=1)
    return embedding


def read_file(file_path):
    """
    Read the content of a file (.txt or .docx).

    Parameters:
        file_path (str): Path to the file.

    Returns:
        str: Content of the file as text.
    """
    if file_path.endswith(".txt"):
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    elif file_path.endswith(".docx"):
        try:
            doc = Document(file_path)
            return "\n".join(paragraph.text for paragraph in doc.paragraphs)
        except Exception as e:
            print(f"Error reading .docx file {file_path}: {e}")
            return ""
    return ""


def find_duplicates(target_file, directories, similarity_threshold=0.9):
    """
    Check for duplicates of the target file in specified directories.

    Parameters:
        target_file (str): Path to the target file.
        directories (list): List of directories to search for duplicates.
        similarity_threshold (float): Cosine similarity threshold to consider files as duplicates.

    Returns:
        dict: Contains duplicate files and their similarity scores.
        float: The lowest similarity score found.
        str: The path of the least similar file.
    """
    duplicates = {}
    lowest_similarity = 1.0  # Initialize with max similarity
    least_similar_file = None

    # Read the target file's content and embedding
    target_text = read_file(target_file)
    if not target_text:
        raise ValueError(f"Failed to read the target file: {target_file}")
    target_embedding = get_embedding(target_text)

    for directory in directories:
        for root, _, files in os.walk(directory):
            for file in files:
                try:
                    file_path = os.path.join(root, file)
                    # Read the file content (supports .txt and .docx)
                    file_text = read_file(file_path)
                    if not file_text:
                        continue

                    file_embedding = get_embedding(file_text)

                    # Calculate cosine similarity
                    similarity = cosine_similarity(
                        target_embedding.numpy(), file_embedding.numpy()
                    )[0][0]

                    # Check if similarity exceeds the threshold
                    if similarity >= similarity_threshold:
                        duplicates[file_path] = similarity

                    # Update the least similar file
                    if similarity < lowest_similarity:
                        lowest_similarity = similarity
                        least_similar_file = file_path

                except Exception as e:
                    # Skip files that can't be read
                    print(f"Error processing {file_path}: {e}")

    return duplicates, lowest_similarity, least_similar_file


# Define paths to check
file1_path = r"C:\Users\aksha\OneDrive\Documents\ak1.docx"  # Can be .txt or .docx
directories_to_search = [
    r"C:\Users\aksha\OneDrive\Documents",
    r"C:\Users\aksha\OneDrive\Desktop",
    r"C:\Users\aksha\Downloads",
]

# Search for duplicates
duplicate_files, lowest_similarity, least_similar_file = find_duplicates(file1_path, directories_to_search)

# Output the results
if duplicate_files:
    print("Duplicate files found:")
    for dup, similarity in duplicate_files.items():
        print(f" - {dup} (Similarity: {similarity:.4f})")
else:
    print("No duplicates found.")
    if least_similar_file:
        print(f"Least similar file found:")
        print(f" - {least_similar_file} (Similarity: {lowest_similarity:.4f})")
